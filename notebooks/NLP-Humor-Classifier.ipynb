{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models and Vectorization Strategies for Text Classification\n",
    "\n",
    "This try-it focuses on weighing the positives and negatives of different estimators and vectorization strategies for a text classification problem.  In order to consider each of these components, you should make use of the `Pipeline` and `GridSearchCV` objects in scikitlearn to try different combinations of vectorizers with different estimators.  For each of these, you also want to use the `.cv_results_` to examine the time for the estimator to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  You are to use the text column to classify whether or not the text was humorous.  It is loaded and displayed below.\n",
    "\n",
    "**Note:** The original dataset contains 200K rows of data. It is best to try to use the full dtaset. If the original dataset is too large for your computer, please use the 'dataset-minimal.csv', which has been reduced to 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../text_data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5 reasons the 2016 election feels so personal</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasco police shot mexican migrant from behind,...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True\n",
       "3      5 reasons the 2016 election feels so personal  False\n",
       "4  Pasco police shot mexican migrant from behind,...  False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "\n",
    "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
    "\n",
    "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
    "\n",
    "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayes</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              best_params best_score\n",
       "model                               \n",
       "Logistic                            \n",
       "Decision Tree                       \n",
       "Bayes                               "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'model': ['Logistic', 'Decision Tree', 'Bayes'], \n",
    "             'best_params': ['', '', ''],\n",
    "             'best_score': ['', '', '']}).set_index('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the Environment and Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mitalibansal/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stemming(text):\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "def lemmatizing(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Prepare a FunctionTransformer for both stemming and lemmatizing\n",
    "stem_transformer = FunctionTransformer(lambda x: x.apply(stemming))\n",
    "lemmatize_transformer = FunctionTransformer(lambda x: x.apply(lemmatizing))\n",
    "\n",
    "# Prepare vectorizers\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Pipelines and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare classifiers\n",
    "logistic = LogisticRegression(max_iter=1000)\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "bayes = MultinomialNB()\n",
    "\n",
    "# Define pipelines\n",
    "pipelines = [\n",
    "    ('logistic_stem_count', Pipeline([\n",
    "        ('stem', stem_transformer),\n",
    "        ('vectorize', count_vectorizer),\n",
    "        ('classify', logistic)\n",
    "    ])),\n",
    "    ('logistic_stem_tfidf', Pipeline([\n",
    "        ('stem', stem_transformer),\n",
    "        ('vectorize', tfidf_vectorizer),\n",
    "        ('classify', logistic)\n",
    "    ])),\n",
    "    ('logistic_lemmatize_count', Pipeline([\n",
    "        ('lemmatize', lemmatize_transformer),\n",
    "        ('vectorize', count_vectorizer),\n",
    "        ('classify', logistic)\n",
    "    ])),\n",
    "    ('logistic_lemmatize_tfidf', Pipeline([\n",
    "        ('lemmatize', lemmatize_transformer),\n",
    "        ('vectorize', tfidf_vectorizer),\n",
    "        ('classify', logistic)\n",
    "    ])),\n",
    "    ('tree_stem_count', Pipeline([\n",
    "        ('stem', stem_transformer),\n",
    "        ('vectorize', count_vectorizer),\n",
    "        ('classify', decision_tree)\n",
    "    ])),\n",
    "    ('tree_stem_tfidf', Pipeline([\n",
    "        ('stem', stem_transformer),\n",
    "        ('vectorize', tfidf_vectorizer),\n",
    "        ('classify', decision_tree)\n",
    "    ])),\n",
    "    ('tree_lemmatize_count', Pipeline([\n",
    "        ('lemmatize', lemmatize_transformer),\n",
    "        ('vectorize', count_vectorizer),\n",
    "        ('classify', decision_tree)\n",
    "    ])),\n",
    "    ('tree_lemmatize_tfidf', Pipeline([\n",
    "        ('lemmatize', lemmatize_transformer),\n",
    "        ('vectorize', tfidf_vectorizer),\n",
    "        ('classify', decision_tree)\n",
    "    ])),\n",
    "    ('bayes_stem_count', Pipeline([\n",
    "        ('stem', stem_transformer),\n",
    "        ('vectorize', count_vectorizer),\n",
    "        ('classify', bayes)\n",
    "    ])),\n",
    "    ('bayes_stem_tfidf', Pipeline([\n",
    "        ('stem', stem_transformer),\n",
    "        ('vectorize', tfidf_vectorizer),\n",
    "        ('classify', bayes)\n",
    "    ])),\n",
    "    ('bayes_lemmatize_count', Pipeline([\n",
    "        ('lemmatize', lemmatize_transformer),\n",
    "        ('vectorize', count_vectorizer),\n",
    "        ('classify', bayes)\n",
    "    ])),\n",
    "    ('bayes_lemmatize_tfidf', Pipeline([\n",
    "        ('lemmatize', lemmatize_transformer),\n",
    "        ('vectorize', tfidf_vectorizer),\n",
    "        ('classify', bayes)\n",
    "    ]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "logistic_stem_count {'classify__C': 1} 0.88474 0.88952 612.2974989414215\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "logistic_stem_tfidf {'classify__C': 10} 0.8844866666666666 0.88906 678.0508050918579\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "logistic_lemmatize_count {'classify__C': 1} 0.8805066666666667 0.88286 213.7924029827118\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "logistic_lemmatize_tfidf {'classify__C': 10} 0.8802333333333334 0.88286 246.9951479434967\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    }
   ],
   "source": [
    "# Perform GridSearchCV for each pipeline\n",
    "results = []\n",
    "\n",
    "for name, pipeline in pipelines:\n",
    "    # Define parameter grid based on classifier\n",
    "    param_grid = {'classify__C': [0.1, 1, 10]} if 'logistic' in name else \\\n",
    "                 {'classify__max_depth': [None, 10, 20]} if 'tree' in name else \\\n",
    "                 {'classify__alpha': [0.01, 0.1, 1.0]}\n",
    "\n",
    "    grid = GridSearchCV(pipeline, param_grid=param_grid, cv=3, verbose=1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    y_pred = grid.predict(X_test)\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    y_prob = grid.predict_proba(X_test)[:, 1]  # Probability of the positive class\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'best_params': grid.best_params_,\n",
    "        'best_score': grid.best_score_,\n",
    "        'fit_time': elapsed_time,\n",
    "        'test_score': acc_score,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'AUC': auc,\n",
    "        'y pred': y_pred,\n",
    "        'y_prob': y_prob\n",
    "    })\n",
    "    print(name, grid.best_params_, grid.best_score_, acc_score, elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DataFrame of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results in a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "sns.lineplot(results_df, x='model', y='fit_time')\n",
    "plt.title('Compare Train Time for models')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to plot\n",
    "metrics = ['best_score', 'test_score', 'precision', 'recall', 'f1', 'AUC']\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Plot each metric\n",
    "for metric in metrics:\n",
    "    sns.lineplot(data=results_df, x='model', y=metric, label=metric)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.title('Compare scores for models')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()  # Adjust layout to make room for the legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 2, figsize=(10, 20))  \n",
    "fig.suptitle('ROC Curves for Models\\n\\n')  # Add a title for the entire figure\n",
    "\n",
    "# Counter to keep track of subplot position\n",
    "plot_count = 0\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    model = row['model']\n",
    "    y_prob = row['y_prob']\n",
    "\n",
    "    # Access current subplot based on counter\n",
    "    ax = axes.flat[plot_count]  # Flattened array for easier access\n",
    "    \n",
    "    # Visualizing ROC Curve\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "    \n",
    "    ax.plot(fpr, tpr)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f\"Model: {model}, AUC = {round(row['AUC'],2)}\")\n",
    "\n",
    "    plot_count += 1\n",
    "\n",
    "# Adjust layout to prevent overlapping elements (optional)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 2, figsize=(10, 20))  \n",
    "fig.suptitle('Confusion Matrix for Models\\n\\n')  # Add a title for the entire figure\n",
    "\n",
    "# Counter to keep track of subplot position\n",
    "plot_count = 0\n",
    "\n",
    "for index, row in results_df.iterrows():\n",
    "    model = row['model']\n",
    "    y_pred = row['y pred'] \n",
    "\n",
    "    # Access current subplot based on counter\n",
    "    ax = axes.flat[plot_count]  # Flattened array for easier access\n",
    "\n",
    "    # Visualizing Confusion Matrix\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\n",
    "    ax.set_title(f\"{model}: Confusion Matrix\")\n",
    "\n",
    "    plot_count += 1\n",
    "\n",
    "# Adjust layout to prevent overlapping elements (optional)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "- **Logistic Regression** models consistently outperform the others, especially when using stemming with count vectorization. These models offer the best balance of precision, recall, F1, and AUC, making them well-suited for text classification tasks that require accurate class identification and discrimination. Although stemming slightly improves metrics, it is more computationally expensive compared to lemmatizing.\n",
    "\n",
    "- **Decision Trees** exhibit strong recall and F1 scores but fall short of logistic regression across all metrics.\n",
    "\n",
    "- **Naive Bayes** models deliver competitive performance with higher computational costs and lower metrics compared to logistic regression.\n",
    "\n",
    "Overall, the `logistic_stem_count` model stands out due to its robust performance across all key metrics. However, if computational efficiency is a concern, the `logistic_lemmatize_count` model is a viable alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
